# tokenize with double newline  but not single newline

import re
from nltk.tokenize import sent_tokenize
# file = 'foo.txt'

def tread(file):
    tread = open(s, 'r').read()
    while '  ' in tread: # reduce whitespace
        tread = tread.replace('  ', ' ') 
    tread = re.split('\\\n\\\n|\\\n\\ \\\n',tread) # treats string between double newline \n\n as a standalone sentence
    tread = map( lambda x: " ".join(x.replace('\n', "").split()),tread) # new line is not treated as a new sentence
    lst=[] # empty list
    for each in tread:
        lst.extend(sent_tokenize(each)) # tokenize 
    return lst
